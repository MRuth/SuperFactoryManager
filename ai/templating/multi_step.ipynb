{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do if key not defined\n",
    "if 'key' not in locals():\n",
    "    key = !op read \"op://private/sfm openai api key/password\"\n",
    "    assert len(key) == 1\n",
    "    key = key[0]\n",
    "assert len(key) == 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel\n",
    "\n",
    "openai.api_key = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunTest(BaseModel):\n",
    "    include_visual_observation: bool\n",
    "    agent_test_code: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test():\n",
    "    path = r\"D:\\Repos\\Minecraft\\Forge\\SuperFactoryManager\\src\\gametest\\java\\ca\\teamdman\\sfm\\ai\\OpenDoorTest.java\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_test(content: str):\n",
    "    path = r\"D:\\Repos\\Minecraft\\Forge\\SuperFactoryManager\\src\\gametest\\java\\ca\\teamdman\\sfm\\ai\\OpenDoorTest.java\"\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        a\n",
      "        b\n",
      "        c\n"
     ]
    }
   ],
   "source": [
    "def indent_text(text, spaces=4):\n",
    "    space_str = \" \" * spaces\n",
    "    return \"\\n\".join(f\"{space_str}{line}\" for line in text.split(\"\\n\"))\n",
    "print(indent_text(\"a\\nb\\nc\",8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_agent_content(test: str, new_content: str) -> str:\n",
    "    \"\"\"\n",
    "    We want to remove any previous attempts at the test\n",
    "\n",
    "    // begin agent code\n",
    "    item.setPos(Vec3.atCenterOf(helper.absolutePos(pressurePlatePos).offset(0,3,0)));\n",
    "    // end agent code\n",
    "\n",
    "    should remove the content between the two comments\n",
    "    \"\"\"\n",
    "    region_start = \"        // begin agent code\"\n",
    "    region_end = \"        // end agent code\"\n",
    "    start = test.find(region_start)\n",
    "    end = test.find(region_end)\n",
    "    return test[:start+len(region_start)] + \"\\n\" + new_content + (\"\\n\" if new_content != \"\" else \"\") + test[end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_content(test: str) -> str:\n",
    "    region_start = \"        // begin agent code\"\n",
    "    region_end = \"        // end agent code\"\n",
    "    start = test.find(region_start)\n",
    "    end = test.find(region_end)\n",
    "    return test[start+len(region_start)+1:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# begin hotkey_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "from time import sleep\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focus_intellij():\n",
    "    try:\n",
    "        title = next(x for x in gw.getAllTitles() if \"TestChambers.java\" in x)\n",
    "        windows = gw.getWindowsWithTitle(title)\n",
    "        assert len(windows) > 0, f\"Window not found: {title}\"\n",
    "        windows[0].activate()\n",
    "    except Exception as e:\n",
    "        print(f\"Error focusing window: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_building():\n",
    "    return pyautogui.pixel(1947, 622) == (95, 173, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_build_output():\n",
    "    # store cursor position\n",
    "    x, y = pyautogui.position()\n",
    "    # click and drag from 2890, 638 to 2895, 638 to select text without activating links\n",
    "    pyautogui.moveTo(2890, 638)\n",
    "    pyautogui.mouseDown()\n",
    "    pyautogui.moveTo(2895, 638)\n",
    "    pyautogui.mouseUp()\n",
    "    pyautogui.hotkey('ctrl', 'a')\n",
    "    pyautogui.hotkey('ctrl', 'c')\n",
    "    # restore cursor position\n",
    "    pyautogui.moveTo(x, y)\n",
    "    return pyperclip.paste().replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_happy_build_output(output):\n",
    "    happy = \"\"\"\n",
    "Executing pre-compile tasks…\n",
    "Running 'before' tasks\n",
    "Checking sources\n",
    "Running 'after' tasks\n",
    "Finished, saving caches…\n",
    "Executing post-compile tasks…\n",
    "Finished, saving caches…\n",
    "    \"\"\".strip()\n",
    "    return output.strip() == happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_reload():\n",
    "    print(\"[Hot Reload] Focusing IntelliJ\")\n",
    "    focus_intellij()\n",
    "    print(\"[Hot Reload] Trigger hot reload\")\n",
    "    pyautogui.hotkey('ctrl', 'alt', 'num0')\n",
    "    print(\"[Hot Reload] Wait for build to finish\")\n",
    "    while is_building():\n",
    "        sleep(0.1)\n",
    "    sleep(0.5)\n",
    "    print(\"[Hot Reload] Copy build output\")\n",
    "    output = get_build_output()\n",
    "    success = is_happy_build_output(output)\n",
    "    print(f\"[Hot Reload] Build output succeeded: {success}\")\n",
    "    return success, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end hotkey_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# begin runner_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "msgdir = Path(\"./messages\")\n",
    "msgdir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test():\n",
    "    print(\"[Run Test] Running test, ensure you ran `/sfm_ai listen` in Minecraft\")\n",
    "    # touch messages/run.txt\n",
    "    (msgdir / \"run.txt\").touch()\n",
    "\n",
    "    from time import sleep\n",
    "    # wait for the file to have test results appended\n",
    "    while not (msgdir / \"run.txt\").stat().st_size > 0:\n",
    "        sleep(0.1)\n",
    "    \n",
    "    # read the file\n",
    "    with open(msgdir / \"run.txt\", \"r\") as f:\n",
    "        results = f.read()\n",
    "\n",
    "    archive_content = get_agent_content(read_test()) + \"\\n\\n\" + results\n",
    "    \n",
    "    # clean up the file by renaming it with the time\n",
    "    from datetime import datetime\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    (msgdir / f\"run_{now}.txt\").write_text(archive_content)\n",
    "    (msgdir / \"run.txt\").unlink()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end runner_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_convo(messages):\n",
    "    content = \"\"\n",
    "    for msg in messages:\n",
    "        content += f\"# ~=~ {msg['role']}\\n{msg['content']}\\n\"\n",
    "        if \"function_call\" in msg:\n",
    "            content += f\"---\\n{msg['function_call']}\\n\"\n",
    "    with open(\"convo.md\", \"w\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"The assistant is tasked with solving puzzles in a Minecraft game test environment, similar to the video game _Portal_. The assistant is presented a game test with some code at the beginning and end of the test that the agent can not change. The agent is responsible for replacing the code in the middle of the test to cause the test to succeed.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Current test content:\\n```java\\n{with_agent_content(read_test(), '')}```\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the AI to propose a new solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing build\n",
      "[Hot Reload] Focusing IntelliJ\n",
      "Error focusing window: \n",
      "[Hot Reload] Trigger hot reload\n",
      "[Hot Reload] Wait for build to finish\n",
      "[Hot Reload] Copy build output\n",
      "[Hot Reload] Build output succeeded: True\n",
      "Build succeeded! Running test Executing pre-compile tasks…\n",
      "Running 'before' tasks\n",
      "Checking sources\n",
      "Running 'after' tasks\n",
      "Finished, saving caches…\n",
      "Executing post-compile tasks…\n",
      "Finished, saving caches…\n",
      "[Run Test] Running test, ensure you ran `/sfm_ai listen` in Minecraft\n",
      "Test failed! 1 tries so far, test output: open_door failed! Expected property open to be true, was false at 2,-58,4 (relative: 2,2,1) (t=60)\n",
      "sleeping just in case\n",
      "Getting the AI to propose a new solution\n",
      "Performing build\n",
      "[Hot Reload] Focusing IntelliJ\n",
      "Error focusing window: \n",
      "[Hot Reload] Trigger hot reload\n",
      "[Hot Reload] Wait for build to finish\n",
      "[Hot Reload] Copy build output\n",
      "[Hot Reload] Build output succeeded: True\n",
      "Build succeeded! Running test Executing pre-compile tasks…\n",
      "Running 'before' tasks\n",
      "Checking sources\n",
      "Running 'after' tasks\n",
      "Finished, saving caches…\n",
      "Executing post-compile tasks…\n",
      "Finished, saving caches…\n",
      "[Run Test] Running test, ensure you ran `/sfm_ai listen` in Minecraft\n",
      "Test passed after 2 tries! open_door passed! (245ms)\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "tries = 0\n",
    "while True:\n",
    "    print(\"Getting the AI to propose a new solution\")\n",
    "    response0 = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        # model=\"gpt-3.5-turbo\",\n",
    "        messages=messages[:2] + messages[-2:], # only send the last 2 messages\n",
    "        functions=[\n",
    "            {\n",
    "            \"name\": \"run_test\",\n",
    "            \"description\": \"Run the test with the agent code block substituted for the provided content. Requesting a visual observation will slow down the process through a dependency on humans.\",\n",
    "            \"parameters\": RunTest.model_json_schema()\n",
    "            },\n",
    "        ],\n",
    "        function_call={\"name\": \"run_test\"}\n",
    "    )\n",
    "    tries += 1\n",
    "    # print(response0)\n",
    "\n",
    "    print(\"Performing build\")\n",
    "    run = RunTest(**json.loads(response0.choices[0][\"message\"][\"function_call\"][\"arguments\"]))\n",
    "    write_test(with_agent_content(read_test(), indent_text(run.agent_test_code, 12)))\n",
    "    build_success, build_output = hot_reload()\n",
    "    if not build_success:\n",
    "        print(\"Build failed!\", build_output)\n",
    "    if build_success:\n",
    "        print(\"Build succeeded! Running test\", build_output)\n",
    "        test_output = run_test()\n",
    "\n",
    "    # update conversation history \n",
    "    fn0 = {\n",
    "        \"role\": \"function\",\n",
    "        \"name\": \"run_test\",\n",
    "        \"content\": build_output if not build_success else test_output,\n",
    "    }\n",
    "    messages.append(response0.choices[0][\"message\"])\n",
    "    messages.append(fn0)\n",
    "    save_convo(messages)\n",
    "\n",
    "    if build_success and \"passed\" in test_output:\n",
    "        print(f\"Test passed after {tries} tries!\", test_output)\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Test failed! {tries} tries so far, test output: {test_output if build_success else None}\")\n",
    "    print(\"sleeping just in case\")\n",
    "    sleep(3)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
