{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = !op read \"op://private/sfm openai api key/password\"\n",
    "assert len(key) == 1\n",
    "key = key[0]\n",
    "assert len(key) == 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel\n",
    "\n",
    "openai.api_key = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunTest(BaseModel):\n",
    "    include_visual_observation: bool\n",
    "    agent_test_code: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test():\n",
    "    path = r\"D:\\Repos\\Minecraft\\Forge\\SuperFactoryManager\\src\\gametest\\java\\ca\\teamdman\\sfm\\ai\\TestChambers.java\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_test(content: str):\n",
    "    path = r\"D:\\Repos\\Minecraft\\Forge\\SuperFactoryManager\\src\\gametest\\java\\ca\\teamdman\\sfm\\ai\\TestChambers.java\"\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        a\n",
      "        b\n",
      "        c\n"
     ]
    }
   ],
   "source": [
    "def indent_text(text, spaces=4):\n",
    "    space_str = \" \" * spaces\n",
    "    return \"\\n\".join(f\"{space_str}{line}\" for line in text.split(\"\\n\"))\n",
    "print(indent_text(\"a\\nb\\nc\",8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_agent_content(test: str, new_content: str) -> str:\n",
    "    \"\"\"\n",
    "    We want to remove any previous attempts at the test\n",
    "\n",
    "    // begin agent code\n",
    "    item.setPos(Vec3.atCenterOf(helper.absolutePos(pressurePlatePos).offset(0,3,0)));\n",
    "    // end agent code\n",
    "\n",
    "    should remove the content between the two comments\n",
    "    \"\"\"\n",
    "    region_start = \"        // begin agent code\"\n",
    "    region_end = \"        // end agent code\"\n",
    "    start = test.find(region_start)\n",
    "    end = test.find(region_end)\n",
    "    return test[:start+len(region_start)] + \"\\n\" + new_content + (\"\\n\" if new_content != \"\" else \"\") + test[end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_content(test: str) -> str:\n",
    "    region_start = \"        // begin agent code\"\n",
    "    region_end = \"        // end agent code\"\n",
    "    start = test.find(region_start)\n",
    "    end = test.find(region_end)\n",
    "    return test[start+len(region_start)+1:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# begin hotkey_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "from time import sleep\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focus_intellij():\n",
    "    try:\n",
    "        title = next(x for x in gw.getAllTitles() if \"TestChambers.java\" in x)\n",
    "        windows = gw.getWindowsWithTitle(title)\n",
    "        assert len(windows) > 0, f\"Window not found: {title}\"\n",
    "        windows[0].activate()\n",
    "    except Exception as e:\n",
    "        print(f\"Error focusing window: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_building():\n",
    "    return pyautogui.pixel(1947, 622) == (95, 173, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_build_output():\n",
    "    # store cursor position\n",
    "    x, y = pyautogui.position()\n",
    "    # click and drag from 2890, 638 to 2895, 638 to select text without activating links\n",
    "    pyautogui.moveTo(2890, 638)\n",
    "    pyautogui.mouseDown()\n",
    "    pyautogui.moveTo(2895, 638)\n",
    "    pyautogui.mouseUp()\n",
    "    pyautogui.hotkey('ctrl', 'a')\n",
    "    pyautogui.hotkey('ctrl', 'c')\n",
    "    # restore cursor position\n",
    "    pyautogui.moveTo(x, y)\n",
    "    return pyperclip.paste().replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_happy_build_output(output):\n",
    "    happy = \"\"\"\n",
    "Executing pre-compile tasks…\n",
    "Running 'before' tasks\n",
    "Checking sources\n",
    "Running 'after' tasks\n",
    "Finished, saving caches…\n",
    "Executing post-compile tasks…\n",
    "Finished, saving caches…\n",
    "    \"\"\".strip()\n",
    "    return output.strip() == happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_reload():\n",
    "    print(\"[Hot Reload] Focusing IntelliJ\")\n",
    "    focus_intellij()\n",
    "    print(\"[Hot Reload] Trigger hot reload\")\n",
    "    pyautogui.hotkey('ctrl', 'alt', 'num0')\n",
    "    print(\"[Hot Reload] Wait for build to finish\")\n",
    "    while is_building():\n",
    "        sleep(0.1)\n",
    "    sleep(0.5)\n",
    "    print(\"[Hot Reload] Copy build output\")\n",
    "    output = get_build_output()\n",
    "    success = is_happy_build_output(output)\n",
    "    print(f\"[Hot Reload] Build output succeeded: {success}\")\n",
    "    return success, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end hotkey_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"The assistant is tasked with solving puzzles in a Minecraft game test environment, similar to the video game _Portal_. The assistant is presented a game test with some code at the beginning and end of the test that the agent can not change. The agent is responsible for replacing the code in the middle of the test to cause the test to succeed.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Current test content:\\n```java\\n{with_agent_content(read_test(), '')}```\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_convo(messages):\n",
    "    content = \"\"\n",
    "    for msg in messages:\n",
    "        content += f\"# ~=~ {msg['role']}\\n{msg['content']}\\n\"\n",
    "        if \"function_call\" in msg:\n",
    "            content += f\"---\\n{msg['function_call']}\\n\"\n",
    "    with open(\"convo.md\", \"w\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7x8DRZYIKKfxcoTniC9rmNTey5Xfe\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1694327289,\n",
      "  \"model\": \"gpt-4-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"run_test\",\n",
      "          \"arguments\": \"{\\n\\\"include_visual_observation\\\": false,\\n\\\"agent_test_code\\\": \\n\\\"// The agent needs to create an action to make the item entity fall, this action will then activate the pressure\\\\n// plate which will in turn open the door.\\\\n// Move the diamond item to the pressure plate\\\\nitem.setPos(Vec3.atCenterOf(helper.absolutePos(pressurePlatePos).offset(0, 0.5, 0)));\\\\n\\\\n// Make item fall\\\\nitem.setDeltaMovement(0, -0.5, 0);\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 1049,\n",
      "    \"completion_tokens\": 114,\n",
      "    \"total_tokens\": 1163\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response0 = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=messages,\n",
    "    functions=[\n",
    "        {\n",
    "          \"name\": \"run_test\",\n",
    "          \"description\": \"Run the test with the agent code block substituted for the provided content. Requesting a visual observation will slow down the process through a dependency on humans.\",\n",
    "          \"parameters\": RunTest.model_json_schema()\n",
    "        },\n",
    "    ],\n",
    "    function_call={\"name\": \"run_test\"}\n",
    ")\n",
    "print(response0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hot Reload] Focusing IntelliJ\n",
      "[Hot Reload] Trigger hot reload\n",
      "[Hot Reload] Wait for build to finish\n",
      "[Hot Reload] Copy build output\n",
      "[Hot Reload] Build output succeeded: False\n",
      "D:\\Repos\\Minecraft\\Forge\\SuperFactoryManager\\src\\gametest\\java\\ca\\teamdman\\sfm\\ai\\TestChambers.java:94: error: incompatible types: possible lossy conversion from double to int\n",
      "            item.setPos(Vec3.atCenterOf(helper.absolutePos(pressurePlatePos).offset(0, 0.5, 0)));\n",
      "                                                                                       ^\n"
     ]
    }
   ],
   "source": [
    "run = RunTest(**json.loads(response0.choices[0][\"message\"][\"function_call\"][\"arguments\"]))\n",
    "write_test(with_agent_content(read_test(), indent_text(run.agent_test_code, 12)))\n",
    "success, build_output = hot_reload()\n",
    "if not success:\n",
    "    print(build_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# begin runner_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "msgdir = Path(\"./messages\")\n",
    "msgdir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test():\n",
    "    print(\"[Run Test] Running test, ensure you ran `/sfm_ai listen` in Minecraft\")\n",
    "    # touch messages/run.txt\n",
    "    (msgdir / \"run.txt\").touch()\n",
    "\n",
    "    from time import sleep\n",
    "    # wait for the file to have test results appended\n",
    "    while not (msgdir / \"run.txt\").stat().st_size > 0:\n",
    "        sleep(0.1)\n",
    "    \n",
    "    # read the file\n",
    "    with open(msgdir / \"run.txt\", \"r\") as f:\n",
    "        results = f.read()\n",
    "\n",
    "    archive_content = get_agent_content(read_test()) + \"\\n\\n\" + results\n",
    "    \n",
    "    # clean up the file by renaming it with the time\n",
    "    from datetime import datetime\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    (msgdir / f\"run_{now}.txt\").write_text(archive_content)\n",
    "    (msgdir / \"run.txt\").unlink()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end runner_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    test_output = run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'function', 'name': 'run_test', 'content': 'D:\\\\Repos\\\\Minecraft\\\\Forge\\\\SuperFactoryManager\\\\src\\\\gametest\\\\java\\\\ca\\\\teamdman\\\\sfm\\\\ai\\\\TestChambers.java:94: error: incompatible types: possible lossy conversion from double to int\\n            item.setPos(Vec3.atCenterOf(helper.absolutePos(pressurePlatePos).offset(0, 0.5, 0)));\\n                                                                                       ^'}\n"
     ]
    }
   ],
   "source": [
    "fn0 = {\n",
    "    \"role\": \"function\",\n",
    "    \"name\": \"run_test\",\n",
    "    \"content\": build_output if not success else test_output,\n",
    "}\n",
    "print(fn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7x8DY1zE4hwULupOJJkWKkHh8CKsC\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1694327296,\n",
      "  \"model\": \"gpt-4-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"run_test\",\n",
      "          \"arguments\": \"{\\n\\\"include_visual_observation\\\": false,\\n\\\"agent_test_code\\\": \\n\\\"// The agent needs to create an action to make the item entity fall, this action will then activate the pressure\\\\n// plate which will in turn open the door.\\\\n// Move the diamond item to the pressure plate\\\\nitem.setPos(Vec3.atCenterOf(helper.absolutePos(pressurePlatePos).offset(0, 1, 0)));\\\\n\\\\n// Make item fall\\\\nitem.setDeltaMovement(0, -0.5, 0);\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 1262,\n",
      "    \"completion_tokens\": 112,\n",
      "    \"total_tokens\": 1374\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response1 = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        *messages,\n",
    "        response0.choices[0][\"message\"],\n",
    "        fn0,\n",
    "    ],\n",
    "    functions=[\n",
    "        {\n",
    "          \"name\": \"run_test\",\n",
    "          \"description\": \"Run the test with the agent code block substituted for the provided content. Requesting a visual observation will slow down the process through a dependency on humans.\",\n",
    "          \"parameters\": RunTest.model_json_schema()\n",
    "        },\n",
    "    ],\n",
    "    function_call={\"name\": \"run_test\"}\n",
    ")\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hot Reload] Focusing IntelliJ\n",
      "[Hot Reload] Trigger hot reload\n",
      "[Hot Reload] Wait for build to finish\n",
      "[Hot Reload] Copy build output\n",
      "[Hot Reload] Build output succeeded: True\n"
     ]
    }
   ],
   "source": [
    "if response1.choices[0][\"message\"][\"content\"] is not None:\n",
    "    print(response1.choices[0][\"message\"][\"content\"])\n",
    "if \"function_call\" in response1.choices[0][\"message\"]:\n",
    "    run = RunTest(**json.loads(response1.choices[0][\"message\"][\"function_call\"][\"arguments\"]))\n",
    "    write_test(with_agent_content(read_test(), indent_text(run.agent_test_code, 12)))\n",
    "    success, build_output = hot_reload()\n",
    "    if not success:\n",
    "        print(build_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run Test] Running test, ensure you ran `/sfm_ai listen` in Minecraft\n"
     ]
    }
   ],
   "source": [
    "if success:\n",
    "    test_output = run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn1 = {\n",
    "    \"role\": \"function\",\n",
    "    \"name\": \"run_test\",\n",
    "    \"content\": build_output if not success else test_output,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7x8DgJY6B8TYJgzKsgK4B2BhkfrGc\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1694327304,\n",
      "  \"model\": \"gpt-4-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"The test ran successfully and passed!\\n\\nHere is the agent code that replaced the previously empty block, which caused the test to succeed:\\n\\n```java\\n// The agent needs to create an action to make the item entity fall, this action will then activate the pressure plate which will in turn open the door.\\n// Move the diamond item to the pressure plate\\nitem.setPos(Vec3.atCenterOf(helper.absolutePos(pressurePlatePos).offset(0, 1, 0)));\\n\\n// Make item fall\\nitem.setDeltaMovement(0, -0.5, 0);\\n```\\nThis code causes the diamond item to drop onto the pressure plate, which triggers the redstone circuit and opens the door. This meets the success condition for the test.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 1390,\n",
      "    \"completion_tokens\": 153,\n",
      "    \"total_tokens\": 1543\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response2 = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        *messages,\n",
    "        response0.choices[0][\"message\"],\n",
    "        fn0,\n",
    "        response1.choices[0][\"message\"],\n",
    "        fn1\n",
    "    ],\n",
    "    functions=[\n",
    "        {\n",
    "          \"name\": \"run_test\",\n",
    "          \"description\": \"Run the test with the agent code block substituted for the provided content. Requesting a visual observation will slow down the process through a dependency on humans.\",\n",
    "          \"parameters\": RunTest.model_json_schema()\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_convo([\n",
    "    *messages,\n",
    "    response0.choices[0][\"message\"],\n",
    "    fn0,\n",
    "    response1.choices[0][\"message\"],\n",
    "    fn1,\n",
    "    response2.choices[0][\"message\"],\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
