{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = !op read \"op://private/sfm openai api key/password\"\n",
    "assert len(key) == 1\n",
    "key = key[0]\n",
    "assert len(key) == 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel\n",
    "\n",
    "openai.api_key = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunTest(BaseModel):\n",
    "    include_visual_observation: bool\n",
    "    agent_test_code: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test():\n",
    "    path = r\"D:\\Repos\\Minecraft\\Forge\\SuperFactoryManager\\src\\gametest\\java\\ca\\teamdman\\sfm\\ai\\TestChambers.java\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_test(content: str):\n",
    "    path = r\"D:\\Repos\\Minecraft\\Forge\\SuperFactoryManager\\src\\gametest\\java\\ca\\teamdman\\sfm\\ai\\TestChambers.java\"\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        a\n",
      "        b\n",
      "        c\n"
     ]
    }
   ],
   "source": [
    "def indent_text(text, spaces=4):\n",
    "    space_str = \" \" * spaces\n",
    "    return \"\\n\".join(f\"{space_str}{line}\" for line in text.split(\"\\n\"))\n",
    "print(indent_text(\"a\\nb\\nc\",8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_agent_content(test: str, new_content: str) -> str:\n",
    "    \"\"\"\n",
    "    We want to remove any previous attempts at the test\n",
    "\n",
    "    // begin agent code\n",
    "    item.setPos(Vec3.atCenterOf(helper.absolutePos(pressurePlatePos).offset(0,3,0)));\n",
    "    // end agent code\n",
    "\n",
    "    should remove the content between the two comments\n",
    "    \"\"\"\n",
    "    region_start = \"        // begin agent code\"\n",
    "    region_end = \"        // end agent code\"\n",
    "    start = test.find(region_start)\n",
    "    end = test.find(region_end)\n",
    "    return test[:start+len(region_start)] + \"\\n\" + new_content + (\"\\n\" if new_content != \"\" else \"\") + test[end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_content(test: str) -> str:\n",
    "    region_start = \"        // begin agent code\"\n",
    "    region_end = \"        // end agent code\"\n",
    "    start = test.find(region_start)\n",
    "    end = test.find(region_end)\n",
    "    return test[start+len(region_start)+1:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# begin hotkey_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "from time import sleep\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focus_intellij():\n",
    "    try:\n",
    "        title = next(x for x in gw.getAllTitles() if \"TestChambers.java\" in x)\n",
    "        windows = gw.getWindowsWithTitle(title)\n",
    "        assert len(windows) > 0, f\"Window not found: {title}\"\n",
    "        windows[0].activate()\n",
    "    except Exception as e:\n",
    "        print(f\"Error focusing window: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_building():\n",
    "    return pyautogui.pixel(1947, 622) == (95, 173, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_build_output():\n",
    "    # store cursor position\n",
    "    x, y = pyautogui.position()\n",
    "    # click and drag from 2890, 638 to 2895, 638 to select text without activating links\n",
    "    pyautogui.moveTo(2890, 638)\n",
    "    pyautogui.mouseDown()\n",
    "    pyautogui.moveTo(2895, 638)\n",
    "    pyautogui.mouseUp()\n",
    "    pyautogui.hotkey('ctrl', 'a')\n",
    "    pyautogui.hotkey('ctrl', 'c')\n",
    "    # restore cursor position\n",
    "    pyautogui.moveTo(x, y)\n",
    "    return pyperclip.paste().replace('\\r', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_happy_build_output(output):\n",
    "    happy = \"\"\"\n",
    "Executing pre-compile tasks…\n",
    "Running 'before' tasks\n",
    "Checking sources\n",
    "Running 'after' tasks\n",
    "Finished, saving caches…\n",
    "Executing post-compile tasks…\n",
    "Finished, saving caches…\n",
    "    \"\"\".strip()\n",
    "    return output.strip() == happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_reload():\n",
    "    print(\"[Hot Reload] Focusing IntelliJ\")\n",
    "    focus_intellij()\n",
    "    print(\"[Hot Reload] Trigger hot reload\")\n",
    "    pyautogui.hotkey('ctrl', 'alt', 'num0')\n",
    "    print(\"[Hot Reload] Wait for build to finish\")\n",
    "    while is_building():\n",
    "        sleep(0.1)\n",
    "    sleep(0.5)\n",
    "    print(\"[Hot Reload] Copy build output\")\n",
    "    output = get_build_output()\n",
    "    success = is_happy_build_output(output)\n",
    "    print(f\"[Hot Reload] Build output succeeded: {success}\")\n",
    "    return success, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end hotkey_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"The assistant is tasked with solving puzzles in a Minecraft game test environment, similar to the video game _Portal_. The assistant is presented a game test with some code at the beginning and end of the test that the agent can not change. The agent is responsible for replacing the code in the middle of the test to cause the test to succeed.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Current test content:\\n```java\\n{with_agent_content(read_test(), '')}```\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_convo(messages):\n",
    "    content = \"\"\n",
    "    for msg in messages:\n",
    "        content += f\"# ~=~ {msg['role']}\\n{msg['content']}\\n\"\n",
    "        if \"function_call\" in msg:\n",
    "            content += f\"---\\n{msg['function_call']}\\n\"\n",
    "    with open(\"convo.md\", \"w\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_convo(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7x81F3RHOsjz9N2KAYNe58e8jsjSL\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1694326533,\n",
      "  \"model\": \"gpt-4-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"run_test\",\n",
      "          \"arguments\": \"{\\n  \\\"include_visual_observation\\\": false,\\n  \\\"agent_test_code\\\": \\\"helper.spawnItem(item);\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 1750,\n",
      "    \"completion_tokens\": 24,\n",
      "    \"total_tokens\": 1774\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response0 = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=messages,\n",
    "    functions=[\n",
    "        {\n",
    "          \"name\": \"run_test\",\n",
    "          \"description\": \"Run the test with the agent code block substituted for the provided content. Requesting a visual observation will slow down the process through a dependency on humans.\",\n",
    "          \"parameters\": RunTest.model_json_schema()\n",
    "        },\n",
    "    ],\n",
    "    function_call={\"name\": \"run_test\"}\n",
    ")\n",
    "print(response0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hot Reload] Focusing IntelliJ\n",
      "[Hot Reload] Trigger hot reload\n",
      "[Hot Reload] Wait for build to finish\n",
      "[Hot Reload] Copy build output\n",
      "[Hot Reload] Build output succeeded: False\n",
      "D:\\Repos\\Minecraft\\Forge\\SuperFactoryManager\\src\\gametest\\java\\ca\\teamdman\\sfm\\ai\\TestChambers.java:103: error: no suitable method found for spawnItem(ItemEntity)\n",
      "            helper.spawnItem(item);\n",
      "                  ^\n",
      "    method GameTestHelper.spawnItem(Item,float,float,float) is not applicable\n",
      "      (actual and formal argument lists differ in length)\n",
      "    method GameTestHelper.spawnItem(Item,BlockPos) is not applicable\n",
      "      (actual and formal argument lists differ in length)\n"
     ]
    }
   ],
   "source": [
    "run = RunTest(**json.loads(response0.choices[0][\"message\"][\"function_call\"][\"arguments\"]))\n",
    "write_test(with_agent_content(read_test(), indent_text(run.agent_test_code, 12)))\n",
    "success, build_output = hot_reload()\n",
    "if not success:\n",
    "    print(build_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# begin runner_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "msgdir = Path(\"./messages\")\n",
    "msgdir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test():\n",
    "    # touch messages/run.txt\n",
    "    (msgdir / \"run.txt\").touch()\n",
    "\n",
    "    from time import sleep\n",
    "    # wait for the file to have test results appended\n",
    "    while not (msgdir / \"run.txt\").stat().st_size > 0:\n",
    "        sleep(0.1)\n",
    "    \n",
    "    # read the file\n",
    "    with open(msgdir / \"run.txt\", \"r\") as f:\n",
    "        results = f.read()\n",
    "\n",
    "    archive_content = get_agent_content(read_test()) + \"\\n\\n\" + results\n",
    "    \n",
    "    # clean up the file by renaming it with the time\n",
    "    from datetime import datetime\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    (msgdir / f\"run_{now}.txt\").write_text(archive_content)\n",
    "    (msgdir / \"run.txt\").unlink()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end runner_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    test_output = run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'function', 'name': 'run_test', 'content': 'D:\\\\Repos\\\\Minecraft\\\\Forge\\\\SuperFactoryManager\\\\src\\\\gametest\\\\java\\\\ca\\\\teamdman\\\\sfm\\\\ai\\\\TestChambers.java:103: error: no suitable method found for spawnItem(ItemEntity)\\n            helper.spawnItem(item);\\n                  ^\\n    method GameTestHelper.spawnItem(Item,float,float,float) is not applicable\\n      (actual and formal argument lists differ in length)\\n    method GameTestHelper.spawnItem(Item,BlockPos) is not applicable\\n      (actual and formal argument lists differ in length)'}\n"
     ]
    }
   ],
   "source": [
    "fn0 = {\n",
    "    \"role\": \"function\",\n",
    "    \"name\": \"run_test\",\n",
    "    \"content\": build_output if not success else test_output,\n",
    "}\n",
    "print(fn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7x82ajsu8nb2FtuOI86K0g1ySR3ku\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1694326616,\n",
      "  \"model\": \"gpt-4-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"run_test\",\n",
      "          \"arguments\": \"{\\n  \\\"include_visual_observation\\\": false,\\n  \\\"agent_test_code\\\": \\\"helper.spawnItem(Items.DIAMOND, pressurePlatePos);\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 1900,\n",
      "    \"completion_tokens\": 32,\n",
      "    \"total_tokens\": 1932\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response1 = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        *messages,\n",
    "        response0.choices[0][\"message\"],\n",
    "        fn0,\n",
    "    ],\n",
    "    functions=[\n",
    "        {\n",
    "          \"name\": \"run_test\",\n",
    "          \"description\": \"Run the test with the agent code block substituted for the provided content. Requesting a visual observation will slow down the process through a dependency on humans.\",\n",
    "          \"parameters\": RunTest.model_json_schema()\n",
    "        },\n",
    "    ],\n",
    "    function_call={\"name\": \"run_test\"}\n",
    ")\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hot Reload] Focusing IntelliJ\n",
      "[Hot Reload] Trigger hot reload\n",
      "[Hot Reload] Wait for build to finish\n",
      "[Hot Reload] Copy build output\n",
      "[Hot Reload] Build output succeeded: True\n"
     ]
    }
   ],
   "source": [
    "if response1.choices[0][\"message\"][\"content\"] is not None:\n",
    "    print(response1.choices[0][\"message\"][\"content\"])\n",
    "if \"function_call\" in response1.choices[0][\"message\"]:\n",
    "    run = RunTest(**json.loads(response1.choices[0][\"message\"][\"function_call\"][\"arguments\"]))\n",
    "    write_test(with_agent_content(read_test(), indent_text(run.agent_test_code, 12)))\n",
    "    success, build_output = hot_reload()\n",
    "    if not success:\n",
    "        print(build_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    test_output = run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn1 = {\n",
    "    \"role\": \"function\",\n",
    "    \"name\": \"run_test\",\n",
    "    \"content\": output if not success else test_output,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7x85jzVFL1jFDB0TG6fhHLK1OwaD6\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1694326811,\n",
      "  \"model\": \"gpt-4-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"The test has passed successfully. The code that I've added to make the test case pass is:\\n\\n```java\\nhelper.spawnItem(Items.DIAMOND, pressurePlatePos);\\n```\\n\\nThe code spawns a diamond item on the position of the pressure plate which, in turn, activates the redstone wire and opens the door.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 1948,\n",
      "    \"completion_tokens\": 67,\n",
      "    \"total_tokens\": 2015\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response2 = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        *messages,\n",
    "        response0.choices[0][\"message\"],\n",
    "        fn0,\n",
    "        response1.choices[0][\"message\"],\n",
    "        fn1\n",
    "    ],\n",
    "    functions=[\n",
    "        {\n",
    "          \"name\": \"run_test\",\n",
    "          \"description\": \"Run the test with the agent code block substituted for the provided content. Requesting a visual observation will slow down the process through a dependency on humans.\",\n",
    "          \"parameters\": RunTest.model_json_schema()\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_convo([\n",
    "    *messages,\n",
    "    response0.choices[0][\"message\"],\n",
    "    fn0,\n",
    "    response1.choices[0][\"message\"],\n",
    "    fn1,\n",
    "    response2.choices[0][\"message\"],\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
