{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 589/589 [00:00<?, ?B/s] \n",
      "Downloading (…)fetensors.index.json: 100%|██████████| 31.4k/31.4k [00:00<00:00, 15.7MB/s]\n",
      "Downloading (…)of-00003.safetensors: 100%|██████████| 9.95G/9.95G [04:34<00:00, 36.2MB/s]\n",
      "Downloading (…)of-00003.safetensors: 100%|██████████| 9.90G/9.90G [04:32<00:00, 36.3MB/s]\n",
      "Downloading (…)of-00003.safetensors: 100%|██████████| 6.18G/6.18G [02:49<00:00, 36.4MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [11:58<00:00, 239.48s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [04:02<00:00, 80.85s/it] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 116/116 [00:00<00:00, 116kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 749kB/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 5.49MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 10.6MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 411/411 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"codellama/CodeLlama-13b-Instruct-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x2dcd6d41f30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "c:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pipe(\u001b[39m\"\u001b[39;49m\u001b[39m2+2=\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:205\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    165\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\pipelines\\base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         )\n\u001b[0;32m   1138\u001b[0m     )\n\u001b[0;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\pipelines\\base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\pipelines\\base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:268\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[0;32m    267\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(input_ids\u001b[39m=\u001b[39minput_ids, attention_mask\u001b[39m=\u001b[39mattention_mask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    269\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\generation\\utils.py:1602\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1585\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[0;32m   1586\u001b[0m         input_ids,\n\u001b[0;32m   1587\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1598\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1599\u001b[0m     )\n\u001b[0;32m   1600\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[0;32m   1601\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1602\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgreedy_search(\n\u001b[0;32m   1603\u001b[0m         input_ids,\n\u001b[0;32m   1604\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1605\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1606\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1607\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1608\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1609\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1610\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1611\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[0;32m   1612\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1613\u001b[0m     )\n\u001b[0;32m   1615\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[0;32m   1616\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\generation\\utils.py:2450\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2447\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2449\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2450\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2451\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2452\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2453\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2454\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2455\u001b[0m )\n\u001b[0;32m   2457\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2458\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:820\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    817\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m    819\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m    821\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    822\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    823\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    824\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    825\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    826\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    827\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    828\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    829\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    830\u001b[0m )\n\u001b[0;32m    832\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    833\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:708\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    701\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    702\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[0;32m    703\u001b[0m         hidden_states,\n\u001b[0;32m    704\u001b[0m         attention_mask,\n\u001b[0;32m    705\u001b[0m         position_ids,\n\u001b[0;32m    706\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[0;32m    709\u001b[0m         hidden_states,\n\u001b[0;32m    710\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    711\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    712\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    713\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    714\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    715\u001b[0m     )\n\u001b[0;32m    717\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    719\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:437\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    435\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    436\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 437\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[0;32m    438\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[0;32m    440\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:220\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    218\u001b[0m     down_proj \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(down_proj)\n\u001b[0;32m    219\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     down_proj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate_proj(x)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mup_proj(x))\n\u001b[0;32m    222\u001b[0m \u001b[39mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipe(\"2+2=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/4d/a7/05c67003d659a0035f2b3a8cf389c1d9645865aee84a73ce99ddab16682f/accelerate-0.22.0-py3-none-any.whl.metadata\n",
      "  Using cached accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\n",
      "Using cached accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://jllllll.github.io/bitsandbytes-windows-webui\n",
      "Collecting bitsandbytes\n",
      "  Downloading https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl (152.7 MB)\n",
      "     ---------------------------------------- 0.0/152.7 MB ? eta -:--:--\n",
      "     -------------------------------------- 0.0/152.7 MB 640.0 kB/s eta 0:03:59\n",
      "     -------------------------------------- 0.0/152.7 MB 653.6 kB/s eta 0:03:54\n",
      "     ---------------------------------------- 0.2/152.7 MB 2.1 MB/s eta 0:01:12\n",
      "     ---------------------------------------- 0.9/152.7 MB 5.5 MB/s eta 0:00:28\n",
      "      -------------------------------------- 2.7/152.7 MB 13.2 MB/s eta 0:00:12\n",
      "     - ------------------------------------- 4.4/152.7 MB 17.4 MB/s eta 0:00:09\n",
      "     - ------------------------------------- 6.1/152.7 MB 20.4 MB/s eta 0:00:08\n",
      "     - ------------------------------------- 7.8/152.7 MB 22.6 MB/s eta 0:00:07\n",
      "     -- ------------------------------------ 9.5/152.7 MB 24.1 MB/s eta 0:00:06\n",
      "     -- ----------------------------------- 11.2/152.7 MB 36.4 MB/s eta 0:00:04\n",
      "     --- ---------------------------------- 12.9/152.7 MB 36.3 MB/s eta 0:00:04\n",
      "     --- ---------------------------------- 14.7/152.7 MB 36.3 MB/s eta 0:00:04\n",
      "     ---- --------------------------------- 16.4/152.7 MB 36.4 MB/s eta 0:00:04\n",
      "     ---- --------------------------------- 18.1/152.7 MB 36.4 MB/s eta 0:00:04\n",
      "     ---- --------------------------------- 19.8/152.7 MB 36.4 MB/s eta 0:00:04\n",
      "     ----- -------------------------------- 21.5/152.7 MB 38.6 MB/s eta 0:00:04\n",
      "     ----- -------------------------------- 23.3/152.7 MB 38.6 MB/s eta 0:00:04\n",
      "     ------ ------------------------------- 25.0/152.7 MB 38.6 MB/s eta 0:00:04\n",
      "     ------ ------------------------------- 26.7/152.7 MB 38.5 MB/s eta 0:00:04\n",
      "     ------- ------------------------------ 28.4/152.7 MB 38.5 MB/s eta 0:00:04\n",
      "     ------- ------------------------------ 30.1/152.7 MB 38.5 MB/s eta 0:00:04\n",
      "     ------- ------------------------------ 31.9/152.7 MB 38.5 MB/s eta 0:00:04\n",
      "     -------- ----------------------------- 33.6/152.7 MB 38.5 MB/s eta 0:00:04\n",
      "     -------- ----------------------------- 35.4/152.7 MB 38.6 MB/s eta 0:00:04\n",
      "     --------- ---------------------------- 37.1/152.7 MB 38.6 MB/s eta 0:00:03\n",
      "     --------- ---------------------------- 38.8/152.7 MB 38.6 MB/s eta 0:00:03\n",
      "     ---------- --------------------------- 40.5/152.7 MB 36.4 MB/s eta 0:00:04\n",
      "     ---------- --------------------------- 42.2/152.7 MB 36.4 MB/s eta 0:00:04\n",
      "     ---------- --------------------------- 43.9/152.7 MB 36.4 MB/s eta 0:00:03\n",
      "     ----------- -------------------------- 45.6/152.7 MB 36.3 MB/s eta 0:00:03\n",
      "     ----------- -------------------------- 47.3/152.7 MB 36.3 MB/s eta 0:00:03\n",
      "     ------------ ------------------------- 49.1/152.7 MB 36.4 MB/s eta 0:00:03\n",
      "     ------------ ------------------------- 50.8/152.7 MB 36.4 MB/s eta 0:00:03\n",
      "     ------------- ------------------------ 52.5/152.7 MB 36.4 MB/s eta 0:00:03\n",
      "     ------------- ------------------------ 54.2/152.7 MB 36.4 MB/s eta 0:00:03\n",
      "     ------------- ------------------------ 55.9/152.7 MB 36.4 MB/s eta 0:00:03\n",
      "     -------------- ----------------------- 57.6/152.7 MB 36.4 MB/s eta 0:00:03\n",
      "     -------------- ----------------------- 59.4/152.7 MB 38.5 MB/s eta 0:00:03\n",
      "     --------------- ---------------------- 61.1/152.7 MB 38.5 MB/s eta 0:00:03\n",
      "     --------------- ---------------------- 62.8/152.7 MB 38.5 MB/s eta 0:00:03\n",
      "     ---------------- --------------------- 64.5/152.7 MB 38.5 MB/s eta 0:00:03\n",
      "     ---------------- --------------------- 66.2/152.7 MB 38.5 MB/s eta 0:00:03\n",
      "     ---------------- --------------------- 67.9/152.7 MB 38.6 MB/s eta 0:00:03\n",
      "     ----------------- -------------------- 69.7/152.7 MB 38.6 MB/s eta 0:00:03\n",
      "     ----------------- -------------------- 71.4/152.7 MB 38.6 MB/s eta 0:00:03\n",
      "     ------------------ ------------------- 73.1/152.7 MB 38.5 MB/s eta 0:00:03\n",
      "     ------------------ ------------------- 74.8/152.7 MB 36.4 MB/s eta 0:00:03\n",
      "     ------------------- ------------------ 76.5/152.7 MB 36.4 MB/s eta 0:00:03\n",
      "     ------------------- ------------------ 78.2/152.7 MB 36.3 MB/s eta 0:00:03\n",
      "     ------------------- ------------------ 79.9/152.7 MB 36.3 MB/s eta 0:00:03\n",
      "     -------------------- ----------------- 81.7/152.7 MB 36.4 MB/s eta 0:00:02\n",
      "     -------------------- ----------------- 83.4/152.7 MB 36.4 MB/s eta 0:00:02\n",
      "     --------------------- ---------------- 85.1/152.7 MB 36.4 MB/s eta 0:00:02\n",
      "     --------------------- ---------------- 86.8/152.7 MB 36.4 MB/s eta 0:00:02\n",
      "     ---------------------- --------------- 88.5/152.7 MB 38.6 MB/s eta 0:00:02\n",
      "     ---------------------- --------------- 90.2/152.7 MB 38.6 MB/s eta 0:00:02\n",
      "     ---------------------- --------------- 91.9/152.7 MB 38.5 MB/s eta 0:00:02\n",
      "     ----------------------- -------------- 93.6/152.7 MB 38.5 MB/s eta 0:00:02\n",
      "     ----------------------- -------------- 95.3/152.7 MB 38.5 MB/s eta 0:00:02\n",
      "     ------------------------ ------------- 97.0/152.7 MB 38.5 MB/s eta 0:00:02\n",
      "     ------------------------ ------------- 98.8/152.7 MB 38.5 MB/s eta 0:00:02\n",
      "     ------------------------ ------------ 100.4/152.7 MB 36.4 MB/s eta 0:00:02\n",
      "     ------------------------ ------------ 102.2/152.7 MB 38.6 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 103.9/152.7 MB 36.4 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 105.6/152.7 MB 36.4 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 107.3/152.7 MB 36.4 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 109.1/152.7 MB 36.4 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 110.8/152.7 MB 36.3 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 112.5/152.7 MB 36.3 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 114.2/152.7 MB 36.4 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 116.0/152.7 MB 36.4 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 117.8/152.7 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------- -------- 119.5/152.7 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 121.1/152.7 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 122.8/152.7 MB 38.6 MB/s eta 0:00:01\n",
      "     ------------------------------ ------ 124.6/152.7 MB 38.5 MB/s eta 0:00:01\n",
      "     ------------------------------ ------ 126.3/152.7 MB 38.5 MB/s eta 0:00:01\n",
      "     ------------------------------- ----- 128.0/152.7 MB 38.5 MB/s eta 0:00:01\n",
      "     ------------------------------- ----- 129.6/152.7 MB 38.5 MB/s eta 0:00:01\n",
      "     ------------------------------- ----- 131.4/152.7 MB 38.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 133.1/152.7 MB 38.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 134.8/152.7 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 136.6/152.7 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 138.2/152.7 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 140.0/152.7 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 141.7/152.7 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 143.4/152.7 MB 36.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 145.2/152.7 MB 36.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 146.9/152.7 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  148.6/152.7 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  150.3/152.7 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  152.0/152.7 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  152.7/152.7 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------------------  152.7/152.7 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------------------  152.7/152.7 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------------------  152.7/152.7 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------------------  152.7/152.7 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------------------  152.7/152.7 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- 152.7/152.7 MB 16.8 MB/s eta 0:00:00\n",
      "Collecting scipy (from bitsandbytes)\n",
      "  Obtaining dependency information for scipy from https://files.pythonhosted.org/packages/70/03/485f73046134400ea25d3cb178c5e6728f9b165f79d09638ecb44ee0e9b1/scipy-1.11.2-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading scipy-1.11.2-cp310-cp310-win_amd64.whl.metadata (59 kB)\n",
      "     ---------------------------------------- 0.0/59.1 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 20.5/59.1 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------- ----- 51.2/59.1 kB 435.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 59.1/59.1 kB 519.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from scipy->bitsandbytes) (1.24.1)\n",
      "Downloading scipy-1.11.2-cp310-cp310-win_amd64.whl (44.0 MB)\n",
      "   ---------------------------------------- 0.0/44.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/44.0 MB 3.3 MB/s eta 0:00:14\n",
      "    --------------------------------------- 0.6/44.0 MB 8.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.9/44.0 MB 15.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 3.6/44.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 5.4/44.0 MB 24.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 7.1/44.0 MB 26.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 8.8/44.0 MB 28.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 10.5/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 12.3/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 14.0/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 15.7/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 17.3/44.0 MB 36.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 19.1/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 20.8/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 22.5/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 24.2/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.9/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 27.6/44.0 MB 38.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 29.3/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 31.0/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 32.8/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 34.5/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 36.2/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.9/44.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.6/44.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.3/44.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.0/44.0 MB 27.3 MB/s eta 0:00:00\n",
      "Installing collected packages: scipy, bitsandbytes\n",
      "Successfully installed bitsandbytes-0.41.1 scipy-1.11.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeLlamaTokenizerFast(name_or_path='codellama/CodeLlama-13b-Instruct-hf', vocab_size=32016, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'additional_special_tokens': ['▁<PRE>', '▁<MID>', '▁<SUF>', '▁<EOT>']}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-13b-Instruct-hf\", device_map=\"auto\")\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:43<00:00, 14.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32016, 5120)\n",
      "    (layers): ModuleList(\n",
      "      (0-39): 40 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=32016, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-13b-Instruct-hf\", device_map=\"auto\", load_in_8bit=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"@GameTest(template = \"3x4x3\")\n",
    "    public static void open_door(GameTestHelper helper) {\n",
    "        // begin test prefix\n",
    "        // This test case presents a wooden pressure plate, a redstone dust, and a door\n",
    "        // The test passes when the door is detected as open\n",
    "        // The code in the test prefix is the programmatic creation of the redstone, dust, door, and floor\n",
    "        // The code in the test suffix is a test assertion that the blockstate of the door is open\n",
    "        var pressurePlatePos = new BlockPos(0, 2, 1);\n",
    "        var redstonePos = new BlockPos(1, 2, 1);\n",
    "        var doorPos = new BlockPos(2, 2, 1);\n",
    "\n",
    "        // set the floor to iron blocks\n",
    "        for (int x = 0; x <= 2; x++) {\n",
    "            for (int z = 0; z <= 2; z++) {\n",
    "                helper.setBlock(new BlockPos(x, 1, z), Blocks.IRON_BLOCK);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // set the pressure plate\n",
    "        helper.setBlock(pressurePlatePos, Blocks.OAK_PRESSURE_PLATE);\n",
    "\n",
    "        // set the redstone dust\n",
    "        helper.setBlock(redstonePos, Blocks.REDSTONE_WIRE);\n",
    "\n",
    "        // set the door\n",
    "        helper.setBlock(doorPos, Blocks.IRON_DOOR);\n",
    "\n",
    "\n",
    "        var item = new ItemEntity(\n",
    "                helper.getLevel(),\n",
    "                redstonePos.getX(),\n",
    "                redstonePos.getY(),\n",
    "                redstonePos.getZ(),\n",
    "                new ItemStack(Items.DIAMOND)\n",
    "        );\n",
    "        item.setDeltaMovement(0, 0, 0);\n",
    "\n",
    "        // did we place the diamond above the correct block?\n",
    "        item.setPos(Vec3.atCenterOf(helper.absolutePos(doorPos).offset(0, 3, 0)));\n",
    "        // end test prefix\n",
    "\n",
    "        // begin agent code\n",
    "        // end agent code\n",
    "\n",
    "        // begin test suffix\n",
    "        helper.getLevel().addFreshEntity(item);\n",
    "        // succeed as soon as possible\n",
    "        helper.succeedWhen(() -> {\n",
    "            helper.assertBlockProperty(doorPos, DoorBlock.OPEN, true);\n",
    "        });\n",
    "        // shorten the timeout to 3 ticks\n",
    "        helper.runAfterDelay(60, () -> {\n",
    "            helper.assertBlockProperty(doorPos, DoorBlock.OPEN, true);\n",
    "            helper.succeed();\n",
    "        });\n",
    "        // end test suffix\n",
    "    }\"\"\"\n",
    "prompt_template=f'''[INST] Write the code to be inserted after `// begin agent code` that will cause the test to pass. Create a hypothesis for why the test is not currently passing, then propose a solution wrapped using ```:\n",
    "{prompt}\n",
    "[/INST]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "Here is a possible solution to the coding problem:\n",
      "```\n",
      "def tell_me_about_ai(prompt):\n",
      "    if prompt == \"Tell me about AI\":\n",
      "        return \"Artificial intelligence (AI) is a field of computer science that focuses on creating intelligent machines that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI systems use a combination of machine learning, natural language processing, and computer vision to perform these tasks. They can be used in a variety of applications, such as virtual assistants, self-driving cars, and medical diagnosis.\"\n",
      "    else:\n",
      "        return \"I'm not sure what you mean by 'Tell me about AI'. Could you please provide more context or clarify your question?\"\n",
      "```\n",
      "This code defines a function `tell_me_about_ai` that takes a string `prompt` as input. If the prompt is \"Tell me about AI\", the function returns a string describing the field of artificial intelligence. If the prompt is anything else, the function returns a string indicating that it is not sure what the user means by \"Tell me about AI\".\n",
      "\n",
      "This code obeys the constraints of the problem, which are:\n",
      "\n",
      "* The function should take a string `prompt` as input.\n",
      "* The function should return a string describing the field of artificial intelligence if the prompt is \"Tell me about AI\".\n",
      "* The function should return a string indicating that it is not sure what the user means by \"Tell me about AI\" if the prompt is anything else.\n",
      "\n",
      "This code passes the example test cases provided in the problem, which are:\n",
      "```\n",
      "assert tell_me_about_ai(\"Tell me about AI\") == \"Artificial intelligence (AI) is a field of computer science that focuses on creating intelligent machines that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI systems use a combination of machine learning, natural language processing, and computer vision to perform these tasks. They can be used in a variety of applications, such as virtual assistants, self-driving cars, and medical diagnosis.\"\n",
      "assert tell_me_about_ai(\"What is AI?\") == \"I'm not sure what\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Write the code to be inserted after `// begin agent code` that will cause the test to pass. Create a hypothesis for why the test is not currently passing, then propose a solution wrapped using ```:\n",
      "@GameTest(template = \"3x4x3\")\n",
      "    public static void open_door(GameTestHelper helper) {\n",
      "        // begin test prefix\n",
      "        // This test case presents a wooden pressure plate, a redstone dust, and a door\n",
      "        // The test passes when the door is detected as open\n",
      "        // The code in the test prefix is the programmatic creation of the redstone, dust, door, and floor\n",
      "        // The code in the test suffix is a test assertion that the blockstate of the door is open\n",
      "        var pressurePlatePos = new BlockPos(0, 2, 1);\n",
      "        var redstonePos = new BlockPos(1, 2, 1);\n",
      "        var doorPos = new BlockPos(2, 2, 1);\n",
      "\n",
      "        // set the floor to iron blocks\n",
      "        for (int x = 0; x <= 2; x++) {\n",
      "            for (int z = 0; z <= 2; z++) {\n",
      "                helper.setBlock(new BlockPos(x, 1, z), Blocks.IRON_BLOCK);\n",
      "            }\n",
      "        }\n",
      "\n",
      "        // set the pressure plate\n",
      "        helper.setBlock(pressurePlatePos, Blocks.OAK_PRESSURE_PLATE);\n",
      "\n",
      "        // set the redstone dust\n",
      "        helper.setBlock(redstonePos, Blocks.REDSTONE_WIRE);\n",
      "\n",
      "        // set the door\n",
      "        helper.setBlock(doorPos, Blocks.IRON_DOOR);\n",
      "\n",
      "\n",
      "        var item = new ItemEntity(\n",
      "                helper.getLevel(),\n",
      "                redstonePos.getX(),\n",
      "                redstonePos.getY(),\n",
      "                redstonePos.getZ(),\n",
      "                new ItemStack(Items.DIAMOND)\n",
      "        );\n",
      "        item.setDeltaMovement(0, 0, 0);\n",
      "\n",
      "        // did we place the diamond above the correct block?\n",
      "        item.setPos(Vec3.atCenterOf(helper.absolutePos(doorPos).offset(0, 3, 0)));\n",
      "        // end test prefix\n",
      "\n",
      "        // begin agent code\n",
      "        // end agent code\n",
      "\n",
      "        // begin test suffix\n",
      "        helper.getLevel().addFreshEntity(item);\n",
      "        // succeed as soon as possible\n",
      "        helper.succeedWhen(() -> {\n",
      "            helper.assertBlockProperty(doorPos, DoorBlock.OPEN, true);\n",
      "        });\n",
      "        // shorten the timeout to 3 ticks\n",
      "        helper.runAfterDelay(60, () -> {\n",
      "            helper.assertBlockProperty(doorPos, DoorBlock.OPEN, true);\n",
      "            helper.succeed();\n",
      "        });\n",
      "        // end test suffix\n",
      "    }\n",
      "[/INST]\n",
      "\n",
      "The test is not passing because the `ItemEntity` is not being placed correctly on top of the door. The `setPos()` method should be called with the position of the center of the block, but it's being called with the position of the upper edge of the block instead. To fix this, you can use the `Vec3.atCenterOf()` method to get the position of the center of the block:\n",
      "```java\n",
      "var item = new ItemEntity(\n",
      "        helper.getLevel(),\n",
      "        Vec3.atCenterOf(helper.absolutePos(doorPos)).y + 0.5f,\n",
      "        redstonePos.getY(),\n",
      "        redstonePos.getZ(),\n",
      "        new ItemStack(Items.DIAMOND)\n",
      ");\n",
      "```\n",
      "This will ensure that the `ItemEntity` is placed directly over the center of the door block, which should trigger the opening of the door.\n"
     ]
    }
   ],
   "source": [
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask\n",
      "  Obtaining dependency information for flask from https://files.pythonhosted.org/packages/fd/56/26f0be8adc2b4257df20c1c4260ddd0aa396cf8e75d90ab2f7ff99bc34f9/flask-2.3.3-py3-none-any.whl.metadata\n",
      "  Downloading flask-2.3.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting Werkzeug>=2.3.7 (from flask)\n",
      "  Obtaining dependency information for Werkzeug>=2.3.7 from https://files.pythonhosted.org/packages/9b/59/a7c32e3d8d0e546a206e0552a2c04444544f15c1da4a01df8938d20c6ffc/werkzeug-2.3.7-py3-none-any.whl.metadata\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from flask) (3.1.2)\n",
      "Collecting itsdangerous>=2.1.2 (from flask)\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting click>=8.1.3 (from flask)\n",
      "  Obtaining dependency information for click>=8.1.3 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting blinker>=1.6.2 (from flask)\n",
      "  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\teamd\\.conda\\envs\\sfm\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.2)\n",
      "Downloading flask-2.3.3-py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.1 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 41.0/96.1 kB 960.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 96.1/96.1 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Downloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "   ---------------------------------------- 0.0/242.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 242.2/242.2 kB 15.5 MB/s eta 0:00:00\n",
      "Installing collected packages: Werkzeug, itsdangerous, click, blinker, flask\n",
      "Successfully installed Werkzeug-2.3.7 blinker-1.6.2 click-8.1.7 flask-2.3.3 itsdangerous-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "c:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\TeamD\\.conda\\envs\\sfm\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request {\"prompts\": [\"[INST] Solve the provided equations:\\n2+2=\\n25*5=[/INST]\"]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [11/Sep/2023 14:45:16] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------RESPONSES----------\n",
      "[{'generated_text': '[INST] Solve the provided equations:\\n2+2=\\n25*5=[/INST]  The first equation is a simple arithmetic equation, and it can be solved by substituting the values on either side of the equal sign.\\n\\n2 + 2 = 4\\n\\nThe second equation is an algebraic equation, and it can be solved by using the distributive property to expand the product inside the parentheses.\\n\\n25 * 5 = 125'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request {\"prompts\": [\"[INST] Solve the provided equations, explain your thoughts then provide each equation including the answer inside a ``` block:\\n2+2=\\n25*5=[/INST]\"]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [11/Sep/2023 14:46:31] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------RESPONSES----------\n",
      "[{'generated_text': '[INST] Solve the provided equations, explain your thoughts then provide each equation including the answer inside a ``` block:\\n2+2=\\n25*5=[/INST]  Sure! Here are the solutions to the provided equations along with my explanations and the answers in a code block:\\n```\\n1. 2 + 2 = 4\\n\\nExplanation: The sum of 2 and 2 is equal to 4.\\n\\nAnswer: 4\\n\\n2. 25 * 5 = 125\\n\\nExplanation: Multiplying 25 by 5 gives us 125.\\n\\nAnswer: 125\\n```'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request {\"prompts\": [\"[INST] Solve the provided equations, explain your thoughts then provide each equation including the answer inside a ``` block:\\n2+2=\\n25*5=[/INST]\"]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [11/Sep/2023 14:47:48] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------RESPONSES----------\n",
      "[{'generated_text': '[INST] Solve the provided equations, explain your thoughts then provide each equation including the answer inside a ``` block:\\n2+2=\\n25*5=[/INST]  Sure! Here are the solutions to the provided equations along with my explanations and the answers in a code block:\\n```\\n1. 2 + 2 = 4\\n\\nExplanation: The sum of 2 and 2 is equal to 4.\\n\\nAnswer: 4\\n\\n2. 25 * 5 = 125\\n\\nExplanation: Multiplying 25 by 5 gives us 125.\\n\\nAnswer: 125\\n```'}]\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request\n",
    "import json\n",
    "app = Flask(__name__)\n",
    "@app.route(\"/\", methods=[\"POST\"])\n",
    "def index():\n",
    "    req = request.get_json()\n",
    "    print(\"request\", json.dumps(req))\n",
    "    # out = generator.generate(\n",
    "    #     # req[\"prompts\"], max_gen_len=512, temperature=temperature, top_p=top_p\n",
    "    #     req[\"prompts\"], max_gen_len=512, temperature=temperature, top_p=top_p\n",
    "    # )\n",
    "    out = pipe(req[\"prompts\"])\n",
    "    print(\"----------RESPONSES----------\")\n",
    "    for x in out:\n",
    "        print(x)\n",
    "    return json.dumps(out)\n",
    "    \n",
    "\n",
    "app.run(host=\"127.0.0.1\", port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
